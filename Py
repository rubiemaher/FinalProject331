# -*- coding: utf-8 -*-
"""FinalProjectCOMP331.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jta4YBvi6D7j10Zw_ytewyJbpnIOjFhL
"""

#all imports here!!!

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import torch

pip install transformers datasets sklearn torch

#load dataset
dataset = load_dataset("christinacdl/clickbait_detection_dataset")

"""# DistilBERT"""

#pretrained model
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)


#tokenization function
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=64)

#tokenize dataset
encoded_dataset = dataset.map(preprocess_function, batched=True)

#prepare model w/ classification head
model = AutoModelForSequenceClassification.from_pretrained(

    model_name,
    num_labels=2,
    id2label={0: "NOT", 1: "CLICKBAIT"},
    label2id={"NOT": 0, "CLICKBAIT": 1},

    )
model.to("cuda")

#define metrics (to visualize the data later)
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, predictions)
    prec = precision_score(labels, predictions)
    rec = recall_score(labels, predictions)
    f1 = f1_score(labels, predictions)
    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}

#training args
training_args = TrainingArguments(
    output_dir="./clickbait_model",
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="steps",
    logging_steps=100,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    fp16=True,
    dataloader_num_workers=2,
)

#trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    compute_metrics=compute_metrics,
)

#train!!!
trainer.train()

#eval
results = trainer.evaluate(encoded_dataset["test"])
print(results)

#use the model for inference!
def classify_title(title):
        device = "cuda" if torch.cuda.is_available() else "cpu"
        inputs = tokenizer(title, return_tensors="pt", truncation=True, padding=True, max_length=64)
        inputs = {k: v.to(device) for k, v in inputs.items()}  # MOVE INPUTS TO GPU
        model.to(device)  # Make sure model is also on GPU

        outputs = model(**inputs)
        logits = outputs.logits.detach().cpu().numpy()
        pred = np.argmax(logits, axis=-1)[0]
        return model.config.id2label[pred]

print(classify_title("You won't believe what happened next!"))

"""# BERT-base"""

model_name1 = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name1)

model1 = AutoModelForSequenceClassification.from_pretrained(
    model_name1,
    num_labels=2,

    id2label={0: "NOT", 1: "CLICKBAIT"},
    label2id={"NOT": 0, "CLICKBAIT": 1},

    )
model1.to("cuda")

trainer1 = Trainer(
    model=model1,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    compute_metrics=compute_metrics,
)

trainer1.train()

results1 = trainer1.evaluate(encoded_dataset["test"])
print(results1)

"""# RoBERTa-base"""

model_name2 = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name1)

model2 = AutoModelForSequenceClassification.from_pretrained(
    model_name1,
    num_labels=2,
    id2label={0: "NOT", 1: "CLICKBAIT"},
    label2id={"NOT": 0, "CLICKBAIT": 1},

    )
model2.to("cuda")

trainer2 = Trainer(
    model=model2,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    compute_metrics=compute_metrics,
)

trainer2.train()

results2 = trainer2.evaluate(encoded_dataset["test"])
print(results2)
